{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This notebook was inspired by {'EFFICIENT VECTOR REPRESENTATION FOR DOCUMENTS THROUGH CORRUPTION' 2017. (Minmin Chen)}\n",
    "    A model is trained to predict the missing word with the help of a document vector.\n",
    "    The Document Vector is optimized by the model during training.\n",
    "    The Document Vector is then used by a neural net to predict sentiment.\n",
    "    Primary Differences from Minmin Chen 2017:\n",
    "        BoW not used. \n",
    "        Context words are fed to LSTM instead of word averagings. \n",
    "        Word Vectors are taken from GLOVE/Spacy.\n",
    "        Corruption is through dropout instead of removing words from BoW model.\n",
    "    Results: Unable to accurately build doc-vector to be sufficient to inform sentiment.\n",
    "    \n",
    "    On a personal note: Brighid will come back to this once finished with other research. It was a fun model to build...\n",
    "        If unesseccarily filled with LSTMS...\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import en_core_web_md\n",
    "import pickle\n",
    "import random\n",
    "import glob\n",
    "nlp = en_core_web_md.load()\n",
    "standard_elements = ['<UNK>','<PAD>','<SOS>','<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab\n",
    "try:\n",
    "    vocab = pickle.load(open('imdb_vocab.pkl','rb'))\n",
    "except:\n",
    "    if 'y' in input('create new vocab & potentially overwrite? (y/n)').lower():\n",
    "        # Step 1: Get Vocab (there will be some modifications to original, as the original is large)\n",
    "        path_to_vocab = 'aclImdb/imdb.vocab'\n",
    "        vocab_bytes = open(path_to_vocab, 'rb').readlines()\n",
    "        vocab_string = [b.decode(\"utf-8\", \"ignore\").replace('\\r','').replace('\\n','') for b in vocab_bytes]\n",
    "        vocab_string[0:5]  # ['the', 'and', 'a', 'of', 'to']\n",
    "\n",
    "        # Step 2: Check each word against spacy to determine if the word is value-added\n",
    "        vocab = {e:i for i,e in enumerate(standard_elements)}\n",
    "        for v in vocab_string:\n",
    "            tokens = nlp(v)\n",
    "            for t in tokens:\n",
    "                if t.has_vector and not t.lemma_ in vocab:\n",
    "                    vocab[t.lemma_] = len(vocab)\n",
    "\n",
    "        pickle.dump(vocab,open('imdb_vocab.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding vocab for model to use\n",
    "#...Not necessary, as model could train an embedding model. However the dataset may be limited.\n",
    "# Look up a vector for a given word\n",
    "def get_vec(word):\n",
    "    if word not in standard_elements:\n",
    "        token = [t for t in nlp(word)][0]\n",
    "        return token.vector\n",
    "    else:\n",
    "        return np.zeros([300],dtype=np.float32)\n",
    "\n",
    "try:\n",
    "    embedding_matrix = pickle.load(open('imdb_embedding_matrx.pkl','rb'))\n",
    "except:\n",
    "    if 'y' in input('create new embedding & potentially overwrite? (y/n)').lower():    \n",
    "        embedding_matrix = np.zeros([len(vocab),300], dtype=np.float32)\n",
    "        for k,v in vocab.items():\n",
    "            vec = get_vec(k)\n",
    "            embedding_matrix[v,:] = vec\n",
    "\n",
    "        pickle.dump(embedding_matrix, open('imdb_embedding_matrx.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data loader\n",
    "# Requirements: Return batches of data, \n",
    "#  where each batch is from a separate review\n",
    "#  where batches do not switch reviews until commanded\n",
    "#  where a cursor keeps track of the batch position within each review\n",
    "# Reasoning: doc vectors are trained per document, \n",
    "#  but optimizing a single model/doc vector on a single doc would be equivalent to a batchsize of 1\n",
    "\n",
    "class data_loader:\n",
    "    \n",
    "    \"\"\"\n",
    "        filenames = list of filenames for each document\n",
    "        sentiments = list of integer labels of sentiments (must match filenames)\n",
    "        batch_size = number of documents per batch\n",
    "        prior_words = number of words considered prior to the target word\n",
    "        posterior_words = number of words considered after the target word\n",
    "    \"\"\"\n",
    "    def __init__(self, filenames, sentiments, batch_size = 100, prior_words = 3, posterior_words = 3):\n",
    "        \n",
    "        # sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.prior_words = prior_words\n",
    "        self.posterior_words = posterior_words\n",
    "\n",
    "        assert type(self.batch_size) == int and type(self.prior_words) == int and type(self.posterior_words) == int\n",
    "        \n",
    "        # total available documents\n",
    "        self.filenames = filenames\n",
    "        self.sentiments = sentiments\n",
    "        \n",
    "        # documents currently used for batch\n",
    "        self.batch_files = []\n",
    "        self.batch_sentiments = []\n",
    "        self.batch_positions = []  # tuples (line, line_pos)\n",
    "        self.batch_documents = []\n",
    "        \n",
    "\n",
    "    # Return list of sentences, converted to integer position\n",
    "    def read_file(self, filename):\n",
    "        txt = open(filename,'rb').read().decode(\"utf-8\", \"ignore\").replace('\\r','').replace('\\n','')\n",
    "        doc = nlp(txt)\n",
    "        sents = []\n",
    "        for s in doc.sents:\n",
    "            sent = []\n",
    "            for token in s:\n",
    "                if token.lemma_ in vocab:\n",
    "                    sent.append(vocab[token.lemma_])\n",
    "                else:\n",
    "                    sent.append(vocab['<UNK>'])\n",
    "            sents.append(sent)\n",
    "        return sents\n",
    "    \n",
    "    # Create new set of batches\n",
    "    def create_batches(self):\n",
    "        \n",
    "        # Initialize/wipe batch files\n",
    "        self.batch_files = []\n",
    "        self.batch_sentiments = []\n",
    "        self.batch_positions = []  # tuples (line, line_pos)\n",
    "        self.batch_documents = []        \n",
    "        \n",
    "        # Select files at random\n",
    "        for i in range(self.batch_size):\n",
    "            f_idx = random.randint(0, len(self.filenames)-1)\n",
    "            self.batch_files.append(self.filenames[f_idx])\n",
    "            self.batch_sentiments.append(self.sentiments[f_idx])\n",
    "            self.batch_positions.append((0,0))\n",
    "            self.batch_documents.append(self.read_file(self.filenames[f_idx]))\n",
    "            \n",
    "            \n",
    "    # Create batch and new position info\n",
    "    def get_batch_for_doc(self, batch_document, batch_position):\n",
    "        \n",
    "        prior_words = [vocab['<PAD>'] for _ in range(self.prior_words)]\n",
    "        posterior_words = [vocab['<PAD>'] for _ in range(self.posterior_words)]\n",
    "        \n",
    "        next_line = batch_position[0]  # Assumed sentence is off sufficient length. Else bump by 1\n",
    "        next_column = batch_position[1] + 1  # The next word in sentence by default\n",
    "        \n",
    "        # Test that next position will not throw index error\n",
    "        if not len(batch_document[next_line]) > next_column: \n",
    "            if not len(batch_document) > next_line + 1:\n",
    "                # Reset position to start of document\n",
    "                next_line = 0\n",
    "                next_column = 0\n",
    "            else:\n",
    "                # Go to next line\n",
    "                next_line += 1\n",
    "                next_column = 0\n",
    "        \n",
    "        # Populate target word\n",
    "        target_word = batch_document[next_line][next_column]\n",
    "        \n",
    "        # Populate prior words\n",
    "        j = self.prior_words\n",
    "        while next_column - j - 1 >= 0 and j > 0:\n",
    "            \n",
    "            # (Note: -1 because the target word should not be included in prior words)\n",
    "            prior_word = batch_document[next_line][next_column - self.prior_words + j - 1]\n",
    "            prior_words[j-1] = prior_word  # -1 to translate to zero index \n",
    "            j -= 1\n",
    "            \n",
    "        # Populate posterior words\n",
    "        # Note: posterior words is reversed, so that the first word shows up last in the index\n",
    "        j = 0\n",
    "        while next_column + j + 1 < len(batch_document[next_line]) and j < self.posterior_words:\n",
    "            \n",
    "            # +1 to avoid using target word in posterior word\n",
    "            posterior_word = batch_document[next_line][next_column + 1 + j]\n",
    "            posterior_words[self.posterior_words - j - 1] = posterior_word  # -1 to convert to posterior_words to 0 index\n",
    "            j+= 1\n",
    "        \n",
    "        return {'posterior_words':posterior_words, 'prior_words':prior_words, 'curr_pos':(next_line, next_column), 'target_word':target_word}\n",
    "    \n",
    "    # Get batch for training\n",
    "    def aggregate_batch(self):\n",
    "        \n",
    "        # Define deliverables for each batch\n",
    "        batch_prior_words = np.zeros([self.batch_size, self.prior_words])\n",
    "        batch_posterior_words = np.zeros([self.batch_size, self.posterior_words])\n",
    "        batch_sentiments = np.zeros([self.batch_size,])\n",
    "        batch_target_words = np.zeros([self.batch_size,])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            # get the batch information for each document\n",
    "            doc_part = self.get_batch_for_doc(self.batch_documents[i], self.batch_positions[i])\n",
    "            self.batch_positions[i] = doc_part['curr_pos']  # Update the position for this doc\n",
    "            \n",
    "            # put doc batch info into aggregate\n",
    "            batch_prior_words[i:] = np.array(doc_part['prior_words'])\n",
    "            batch_posterior_words[i:] = np.array(doc_part['posterior_words'])\n",
    "            batch_sentiments[i] = self.batch_sentiments[i]\n",
    "            batch_target_words[i] = doc_part['target_word'] \n",
    "\n",
    "        return {'prior':batch_prior_words, 'posterior':batch_posterior_words, 'sentiment':batch_sentiments, 'target_word':batch_target_words}\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize Data Loaders for unsupervised, supervised, and test\n",
    "neg_files = glob.glob('aclImdb\\\\train\\\\neg\\\\*.txt')\n",
    "pos_files = glob.glob('aclImdb\\\\train\\\\pos\\\\*.txt')\n",
    "unsup_files = glob.glob('aclImdb\\\\train\\\\unsup\\\\*.txt')\n",
    "\n",
    "train_unsup_files = neg_files + pos_files + unsup_files\n",
    "train_unsup_sentiments = [0 for _ in neg_files] + [1 for _ in pos_files] + [-1 for _ in unsup_files]\n",
    "assert len(train_unsup_files) == len(train_unsup_sentiments)\n",
    "assert len(train_unsup_files) == 75000\n",
    "\n",
    "train_sup_files = neg_files + pos_files\n",
    "train_sup_sentiments = [0 for _ in neg_files] + [1 for _ in pos_files]\n",
    "assert len(train_sup_files) == 25000\n",
    "assert len(train_sup_files) == len(train_sup_sentiments)\n",
    "\n",
    "test_neg_files = glob.glob('aclImdb\\\\test\\\\neg\\\\*.txt')\n",
    "test_pos_files = glob.glob('aclImdb\\\\test\\\\pos\\\\*.txt')\n",
    "test_files = test_neg_files + test_pos_files\n",
    "test_sentiments = [0 for _ in test_neg_files] + [1 for _ in test_pos_files]\n",
    "assert len(test_files) == 25000\n",
    "assert len(test_files) == len(test_sentiments)\n",
    "\n",
    "prior_words = 5\n",
    "posterior_words = 5\n",
    "\n",
    "dl_train_unsup = data_loader(train_unsup_files, train_unsup_sentiments, prior_words = prior_words, posterior_words = posterior_words)\n",
    "dl_train_sup = data_loader(train_sup_files, train_sup_sentiments, prior_words = prior_words, posterior_words = posterior_words)\n",
    "dl_test = data_loader(test_files, test_sentiments, prior_words = prior_words, posterior_words = posterior_words)\n",
    "# call methods below to move batches and to switch documents\n",
    "# dl.create_batches()\n",
    "# dl.aggregate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build Model\n",
    "# Provide embedding matrix as input (alternatively could train one, if dataset is sufficiently large)\n",
    "def build_lstm_doc_vec_model(batch_size, vocab_size = len(vocab), embedding_size = 300, clip_norm = 1, learning_rate = .001, \n",
    "                             num_sentiments = 2, doc_vec_steps = 3, prior_steps = 3, posterior_steps = 3, dropout=.5):\n",
    "    \n",
    "    # sub-routine to generate lstm model\n",
    "    def lstm_sub( emb_inp, dropout, n_hidden, n_layers = 1):\n",
    "        cells = []\n",
    "        for _ in range(n_layers):\n",
    "            cell = tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                    cell, output_keep_prob=1.0 - dropout)\n",
    "            cells.append(cell)\n",
    "        rnn_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "\n",
    "        outputs, state = tf.nn.dynamic_rnn(rnn_cell, \n",
    "                                            emb_inp, \n",
    "                                            time_major=False,\n",
    "                                            dtype=tf.float32)\n",
    "        c = tf.identity(state[0].c,'c')\n",
    "        h = tf.identity(state[0].h,'h')\n",
    "\n",
    "        return c, h\n",
    "\n",
    "    \n",
    "    with tf.variable_scope('inputs'):\n",
    "        batch_var = tf.placeholder(shape=[], name='batch_size', dtype=tf.int32)\n",
    "        prior_words = tf.placeholder(shape=[None, prior_steps], name='prior_words', dtype=tf.int32,)\n",
    "        posterior_words = tf.placeholder(shape=[None, posterior_steps], name='posterior_words', dtype=tf.int32)\n",
    "        target_word = tf.placeholder(shape=[None], name='target_word', dtype=tf.int32)  # Only used when training lstms & doc vec\n",
    "        target_sentiment = tf.placeholder(shape=[None], name='target_sentiment', dtype=tf.int32)  # Only used when training sentiment engine\n",
    "        embedding_matrix = tf.placeholder(shape=[vocab_size, embedding_size], name='embedding_matrix', dtype=tf.float32)\n",
    "        \n",
    "        feed_dict = {'batch_size':batch_var, 'prior_words':prior_words, 'posterior_words':posterior_words, 'target_word':target_word,\n",
    "                    'target_sentiment':target_sentiment, 'embedding_matrix':embedding_matrix}\n",
    "    \n",
    "    with tf.variable_scope('doc_vector'):\n",
    "        doc_vector = tf.get_variable('batch_doc_vectors',[batch_size, doc_vec_steps, embedding_size],dtype=tf.float32)\n",
    "        doc_vector = tf.nn.dropout(doc_vector, keep_prob=1-dropout, name='batch_doc_vectors_w_dropout')\n",
    "        dv_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "        with tf.variable_scope('lstm'):\n",
    "            doc_c, doc_h = lstm_sub( emb_inp=doc_vector, dropout=.01, n_hidden=1000, n_layers = 1)\n",
    "    \n",
    "    with tf.variable_scope('prior_words'):\n",
    "        prior_words_embedded = tf.nn.embedding_lookup(embedding_matrix, prior_words)\n",
    "        \n",
    "        with tf.variable_scope('lstm'):\n",
    "            prior_c, prior_h = lstm_sub( emb_inp=prior_words_embedded, dropout=.01, n_hidden=1000, n_layers = 1)\n",
    "\n",
    "    with tf.variable_scope('posterior_words'):\n",
    "        posterior_words_embedded = tf.nn.embedding_lookup(embedding_matrix, posterior_words)\n",
    "        \n",
    "        with tf.variable_scope('lstm'):\n",
    "            posterior_c, posterior_h = lstm_sub( emb_inp=posterior_words_embedded, dropout=.01, n_hidden=1000, n_layers = 1)\n",
    "    \n",
    "    with tf.variable_scope('combiner'):\n",
    "        doc_weights = tf.get_variable('doc_weights_lstm',[1000,1000],dtype=tf.float32)\n",
    "        prior_weights = tf.get_variable('prior_weights_lstm',[1000,1000],dtype=tf.float32)\n",
    "        posterior_weights = tf.get_variable('posterior_weights_lstm',[1000,1000],dtype=tf.float32)\n",
    "        \n",
    "        combined = tf.matmul(doc_h,doc_weights) + tf.matmul(prior_h,prior_weights) + tf.matmul(posterior_h, posterior_weights)\n",
    "        combined_drpout = tf.nn.dropout(combined, keep_prob = .95, name='combined_with_dropout')\n",
    "        \n",
    "        with tf.variable_scope('additional_modeling'):\n",
    "            comb_weights = tf.get_variable('combine_weights',[1000,1000],dtype=tf.float32)\n",
    "            comb_bias = tf.get_variable('combine_bias',[1000],dtype=tf.float32)\n",
    "            pred_h = tf.nn.relu(tf.matmul(combined_drpout, comb_weights) + comb_bias)\n",
    "        \n",
    "        #combined = tf.concat([doc_h, \n",
    "        #                      prior_h, \n",
    "        #                      posterior_h], \n",
    "        #                     axis=1, name='combined')\n",
    "        #combined_expanded = tf.reshape(combined,[-1,3,1000])\n",
    "        \n",
    "        #with tf.variable_scope('lstm'):\n",
    "        #    pred_c, pred_h = lstm_sub( emb_inp=combined_expanded, dropout=.01, n_hidden=1000, n_layers = 1)\n",
    "        #\n",
    "        with tf.variable_scope('convert_size'):\n",
    "            combiner_weight = tf.get_variable('combiner_weight',[1000, vocab_size],dtype=tf.float32)\n",
    "            pred_h_converted = tf.matmul( tf.cast(pred_h, dtype=tf.float32), combiner_weight )\n",
    "            \n",
    "    with tf.variable_scope('doc-vec-loss'):\n",
    "        dvm_loss = tf.cast(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_word, logits=pred_h_converted),dtype=tf.float32)\n",
    "        train_dvm_loss = tf.divide(tf.reduce_sum(dvm_loss) , tf.cast(batch_var, tf.float32),\n",
    "                                                            name='train_loss')\n",
    "    with tf.variable_scope('doc-vec-model-optimizer'):\n",
    "        dvm_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        dvm_gradients = tf.gradients(train_dvm_loss, dvm_params)\n",
    "        dvm_clipped_gradients, _ = tf.clip_by_global_norm(dvm_gradients, clip_norm)    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        dvm = optimizer.apply_gradients(zip(dvm_clipped_gradients, dvm_params))\n",
    "        \n",
    "    with tf.variable_scope('doc_vec_opt'):\n",
    "        dv_gradients = tf.gradients(train_dvm_loss, dv_params)\n",
    "        dv_clipped_gradients, _ = tf.clip_by_global_norm(dv_gradients, clip_norm)    \n",
    "        optimizer2 = tf.train.AdamOptimizer(learning_rate)\n",
    "        dv = optimizer2.apply_gradients(zip(dv_clipped_gradients, dv_params))\n",
    "        \n",
    "    doc_vector_model = {'loss':train_dvm_loss, 'dv_opt':dv, 'dvm_opt':dvm}\n",
    "        \n",
    "    with tf.variable_scope('sentiment_analysis'):\n",
    "        \n",
    "        with tf.variable_scope('simple-weight'):\n",
    "\n",
    "            with tf.variable_scope('read-doc-vec', reuse = False):\n",
    "                flat_doc_vector = tf.reshape(doc_vector, shape=[-1, doc_vec_steps * embedding_size])\n",
    "                weights = tf.get_variable('weights',shape=[doc_vec_steps * embedding_size, num_sentiments])\n",
    "                sentiment_predictions = tf.matmul(flat_doc_vector, weights)\n",
    "\n",
    "            with tf.variable_scope('loss-opt'):\n",
    "                sentiment_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_sentiment, logits=sentiment_predictions)\n",
    "                train_sentiment_loss = tf.divide(tf.reduce_sum(sentiment_loss) , tf.cast(batch_var, tf.float32),\n",
    "                                                                name='sentiment_loss')\n",
    "                sentiment_params = [weights]\n",
    "                sentiment_gradients = tf.gradients(train_sentiment_loss, sentiment_params)\n",
    "                sentiment_clipped_gradients = tf.clip_by_global_norm(sentiment_gradients, clip_norm)\n",
    "\n",
    "                with tf.variable_scope('SGD', reuse = False):\n",
    "                    optimizer3 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                    sentiment_update_step = optimizer3.minimize(train_sentiment_loss, var_list = sentiment_params)\n",
    "                \n",
    "        with tf.variable_scope('lstm-small'):\n",
    "            \n",
    "            pre_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            \n",
    "            sentiment_c, sentiment_h = lstm_sub( emb_inp=doc_vector, dropout=0.01, n_hidden=1000, n_layers = 1)\n",
    "            lstm_conversion_weights = tf.get_variable('weights',shape=[1000,2],dtype=tf.float32)\n",
    "            lstm_pred = tf.matmul(sentiment_h, lstm_conversion_weights)\n",
    "            \n",
    "            with tf.variable_scope('loss-opt'):\n",
    "                \n",
    "                sentiment_lstm_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_sentiment, logits=lstm_pred)\n",
    "                train_sentiment_lstm_loss = tf.divide(tf.reduce_sum(sentiment_lstm_loss) , tf.cast(batch_var, tf.float32),\n",
    "                                                                name='sentiment_loss')\n",
    "                \n",
    "                post_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "                sentiment_lstm_params = [pp for pp in post_params if pp not in pre_params]\n",
    "                \n",
    "                with tf.variable_scope('SGD', reuse = False):\n",
    "                    \n",
    "                    optimizer4 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                    sentiment_lstm_update_step = optimizer3.minimize(train_sentiment_lstm_loss, var_list = sentiment_lstm_params)\n",
    "\n",
    "        with tf.variable_scope('relu'):\n",
    "            \n",
    "            relu_nhidden = 1000#50\n",
    "            relu_weights_in = tf.get_variable('weights_in',shape=[doc_vec_steps * embedding_size, relu_nhidden])\n",
    "            b_in = tf.get_variable('bias_in',shape=[relu_nhidden])\n",
    "            relu_weights_out = tf.get_variable('weights_out',shape=[relu_nhidden, num_sentiments])\n",
    "            b_out = tf.get_variable('bias_out',shape=[num_sentiments])\n",
    "            \n",
    "            sentiment_relu_prediction = tf.matmul( tf.nn.relu( tf.matmul(flat_doc_vector, relu_weights_in) + b_in ) , relu_weights_out) + b_out\n",
    "            \n",
    "            with tf.variable_scope('loss'):\n",
    "                sentiment_relu_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_sentiment, logits=sentiment_relu_prediction)\n",
    "                train_sentiment_relu_loss = tf.divide(tf.reduce_sum(sentiment_relu_loss) , tf.cast(batch_var, tf.float32),\n",
    "                                                                    name='sentiment_loss')\n",
    "                \n",
    "                sentiment_relu_params = [relu_weights_in, b_in, relu_weights_out, b_out]\n",
    "                \n",
    "                with tf.variable_scope('SGD', reuse = False):\n",
    "                    \n",
    "                    optimizer5 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                    sentiment_relu_update_step = optimizer5.minimize(train_sentiment_relu_loss, var_list = sentiment_relu_params)\n",
    "\n",
    "    \n",
    "    sentiment_model = {'simple_loss': train_sentiment_loss, 'simple_opt':sentiment_update_step, 'simple_pred':sentiment_predictions,\n",
    "                      'lstm_loss':train_sentiment_lstm_loss, 'lstm_opt':sentiment_lstm_update_step, 'lstm_pred':lstm_pred,\n",
    "                      'relu_loss':train_sentiment_relu_loss, 'relu_opt':sentiment_relu_update_step, 'relu_pred':sentiment_relu_prediction}\n",
    "    \n",
    "    return {'sentiment_model':sentiment_model, 'doc_vector_model':doc_vector_model, 'feed_dict':feed_dict}\n",
    "            \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 contd: Build Model (Moved to below cell as part of generating new saver)\n",
    "with tf.variable_scope('doc-vecs_lstm-madness2'):\n",
    "    docs_per_batch = batch_size = 100  # required to set document-vector size within model\n",
    "    model = build_lstm_doc_vec_model(docs_per_batch, prior_steps = prior_words, posterior_steps = posterior_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from doc_vec_lstm_model_t9/model\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train & Test Model\n",
    "# ... First train unsupervised part (dvm)\n",
    "# ... Then train supervised sentiment part (simple, lstm, relu)\n",
    "# ... Then test entire model to check performance\n",
    "sess = tf.Session()\n",
    "\n",
    "model_directory = 'doc_vec_lstm_model_t9/model'\n",
    "try:\n",
    "    saver = tf.train.Saver()#tf.train.import_meta_graph(model_directory+'.meta')\n",
    "    saver.restore(sess, model_directory)#, tf.train.latest_checkpoint(model_directory.replace('/model','')))\n",
    "except:\n",
    "    if 'y' in input('run new model? (y/n)').lower():\n",
    "        # Step 4 contd: Build Model\n",
    "        #with tf.variable_scope('doc-vecs_lstm-madness2'):\n",
    "        #    docs_per_batch = batch_size = 100  # required to set document-vector size within model\n",
    "        #    model = build_lstm_doc_vec_model(docs_per_batch)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(max_to_keep=4)\n",
    "    else:\n",
    "        assert 1 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupervised 5.228206634521484\n",
      "unsupervised 4.949830532073975\n",
      "unsupervised 5.045355319976807\n",
      "unsupervised 4.598198413848877\n",
      "unsupervised 4.399217128753662\n",
      "unsupervised 4.756768703460693\n",
      "unsupervised 5.021799087524414\n",
      "unsupervised 4.81366491317749\n",
      "unsupervised 4.413426399230957\n",
      "unsupervised 4.345880508422852\n",
      "unsupervised 4.425302028656006\n",
      "unsupervised 5.352427959442139\n",
      "unsupervised 4.593008518218994\n",
      "unsupervised 4.650392055511475\n",
      "unsupervised 4.724662780761719\n",
      "unsupervised 4.469225883483887\n",
      "unsupervised 4.9506049156188965\n",
      "unsupervised 4.677937984466553\n",
      "unsupervised 4.311498641967773\n",
      "unsupervised 4.880538463592529\n",
      "unsupervised 4.6603617668151855\n",
      "unsupervised 4.173721790313721\n",
      "unsupervised 4.723748207092285\n",
      "unsupervised 4.347041130065918\n",
      "unsupervised 4.7895307540893555\n",
      "unsupervised 5.589871883392334\n",
      "unsupervised 4.916631698608398\n",
      "unsupervised 4.511536121368408\n",
      "unsupervised 4.441699981689453\n",
      "unsupervised 4.7048163414001465\n",
      "unsupervised 5.0121049880981445\n",
      "unsupervised 4.698641300201416\n",
      "unsupervised 4.341871738433838\n",
      "unsupervised 4.036534309387207\n",
      "unsupervised 5.352822303771973\n",
      "unsupervised 4.9545207023620605\n",
      "unsupervised 4.922595500946045\n",
      "unsupervised 4.339676380157471\n",
      "unsupervised 4.889289855957031\n",
      "unsupervised 4.175293445587158\n",
      "unsupervised 4.633559226989746\n",
      "unsupervised 4.656942367553711\n",
      "unsupervised 4.445601940155029\n",
      "unsupervised 4.752226829528809\n",
      "unsupervised 5.060642719268799\n",
      "unsupervised 4.855166435241699\n",
      "unsupervised 4.468073844909668\n",
      "unsupervised 4.101144790649414\n",
      "unsupervised 4.874793529510498\n",
      "unsupervised 4.018753528594971\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (100, 3) for Tensor 'doc-vecs_lstm-madness2/inputs/prior_words:0', which has shape '(?, 5)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8147053dd476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m             loss, _ = sess.run( [model['doc_vector_model']['loss'],\n\u001b[1;32m     44\u001b[0m                    model['doc_vector_model']['dv_opt']],\n\u001b[0;32m---> 45\u001b[0;31m                 feed_dict = feed_dict)\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[1;31m#print('doc_vector {}'.format(loss))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\jr530d\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\jr530d\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1101\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (100, 3) for Tensor 'doc-vecs_lstm-madness2/inputs/prior_words:0', which has shape '(?, 5)'"
     ]
    }
   ],
   "source": [
    "# training info\n",
    "num_unsup_epochs = 1000#100000\n",
    "num_sup_epochs = 1000#10000\n",
    "num_test_epochs = 100\n",
    "runs_till_new_docs = 10#0\n",
    "num_to_opt_doc_vec = 30\n",
    "\n",
    "\n",
    "def get_feed_dict(batch):\n",
    "    feed_dict = {model['feed_dict']['batch_size']:batch_size, \n",
    "                 model['feed_dict']['prior_words']:batch['prior'], \n",
    "                 model['feed_dict']['posterior_words']:batch['posterior'], \n",
    "                 model['feed_dict']['target_word']:batch['target_word'],\n",
    "                model['feed_dict']['target_sentiment']:batch['sentiment'], \n",
    "                 model['feed_dict']['embedding_matrix']:embedding_matrix}\n",
    "    return feed_dict\n",
    "\n",
    "# Unsupervised Epochs\n",
    "dl_train_unsup.create_batches()\n",
    "for i in range(num_unsup_epochs):\n",
    "    for j in range(runs_till_new_docs):\n",
    "        b = dl_train_unsup.aggregate_batch()\n",
    "        feed_dict = get_feed_dict(b)\n",
    "        loss, _ = sess.run( [model['doc_vector_model']['loss'],\n",
    "                   model['doc_vector_model']['dvm_opt']],\n",
    "                feed_dict = feed_dict)\n",
    "    if i%20 == 0:\n",
    "        print('unsupervised {}'.format(loss))\n",
    "        saver.save(sess, model_directory)\n",
    "        dl_train_unsup.create_batches()\n",
    "\n",
    "# Supervised Epochs\n",
    "dl_train_sup = data_loader(train_sup_files, train_sup_sentiments)\n",
    "dl_train_sup.create_batches()\n",
    "for i in range(num_sup_epochs):\n",
    "    #for j in range(runs_till_new_docs):\n",
    "        \n",
    "        # Optimize Doc Vector for batch\n",
    "        for k in range(num_to_opt_doc_vec):\n",
    "            \n",
    "            b = dl_train_sup.aggregate_batch()\n",
    "            feed_dict = get_feed_dict(b)\n",
    "            loss, _ = sess.run( [model['doc_vector_model']['loss'],\n",
    "                   model['doc_vector_model']['dv_opt']],\n",
    "                feed_dict = feed_dict)\n",
    "        #print('doc_vector {}'.format(loss))\n",
    "        saver.save(sess, model_directory)\n",
    "            \n",
    "        # Optimize Sentiment models for doc_vector\n",
    "        for k in range(runs_till_new_docs * 4):\n",
    "            b = dl_train_sup.aggregate_batch()\n",
    "            feed_dict = get_feed_dict(b)\n",
    "            simple_loss, _ = sess.run( [model['sentiment_model']['simple_loss'],\n",
    "                   model['sentiment_model']['simple_opt']],\n",
    "                feed_dict = feed_dict)\n",
    "            lstm_loss, _ = sess.run( [model['sentiment_model']['lstm_loss'],\n",
    "                   model['sentiment_model']['lstm_opt']],\n",
    "                feed_dict = feed_dict)\n",
    "            relu_loss, _ = sess.run( [model['sentiment_model']['relu_loss'],\n",
    "                   model['sentiment_model']['relu_opt']],\n",
    "                feed_dict = feed_dict)\n",
    "        print('simple {}\\tlstm {}\\trelu {}'.format(simple_loss, lstm_loss, relu_loss))\n",
    "        saver.save(sess, model_directory)\n",
    "        dl_train_sup.create_batches()\n",
    "    \n",
    "\n",
    "# Test\n",
    "losses = {'simple':0, 'relu':0, 'lstm':0, 'counts':0}\n",
    "dl_test.create_batches()\n",
    "for i in range(num_test_epochs):\n",
    "\n",
    "        # Optimize Doc Vector for batch\n",
    "        for k in range(num_to_opt_doc_vec):\n",
    "            \n",
    "            b = dl_test.aggregate_batch()\n",
    "            feed_dict = get_feed_dict(b)\n",
    "            loss, _ = sess.run( [model['doc_vector_model']['loss'],\n",
    "                   model['doc_vector_model']['dv_opt']],\n",
    "                feed_dict = feed_dict)\n",
    "        print('doc_vector {}'.format(loss))\n",
    "        saver.save(sess, model_directory)\n",
    "            \n",
    "        # Calculate Loss and return predictions\n",
    "        for k in range(runs_till_new_docs):\n",
    "            b = dl_test.aggregate_batch()\n",
    "            feed_dict = get_feed_dict(b)\n",
    "            simple_loss = sess.run( [model['sentiment_model']['simple_loss']],\n",
    "                feed_dict = feed_dict)[0]\n",
    "            lstm_loss = sess.run( [model['sentiment_model']['simple_loss']],\n",
    "                feed_dict = feed_dict)[0]\n",
    "            relu_loss = sess.run( [model['sentiment_model']['relu_loss']],\n",
    "                feed_dict = feed_dict)[0]\n",
    "            losses['simple'] += simple_loss\n",
    "            losses['relu'] += relu_loss\n",
    "            losses['lstm'] += lstm_loss\n",
    "            losses['counts'] += 1\n",
    "        print('simple {}\\tlstm {}\\trelu {}'.format(simple_loss, lstm_loss, relu_loss))\n",
    "        saver.save(sess, model_directory)\n",
    "        dl_test.create_batches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
